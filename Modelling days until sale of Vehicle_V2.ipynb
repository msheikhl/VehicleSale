{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of vehicle sale timline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split , learning_curve,ShuffleSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor,RandomForestRegressor, ExtraTreesRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "np.random.seed(31415)# for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'xyz.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3fd84f47cdf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xyz.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'xyz.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#importing the dataset\n",
    "warnings.filterwarnings('ignore')\n",
    "data0=pd.read_csv('xyz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the data and some explorartory analysis\n",
    "data0.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to transform sale_class,year,month,weekday,dayofMonth to categorical variables \n",
    "\n",
    "def to_categorical(df):\n",
    "    for index in df.columns:\n",
    "        if index in ['sale_class','year','Month(date_added)','weekday(date_added)','dayOfMonth(date_added)']:\n",
    "            df[index]=df[index].astype('category')\n",
    "    return df\n",
    "\n",
    "data1=to_categorical(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our data set doesn't have any  null values which makes preprocessing easier\n",
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of the outcome variable daysOnWebsite for both used and new sale classes \n",
    "#initiate the Plotly Notebook mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "traceAll={\"x\":data1['daysOnWeb'],\"type\":\"box\",\"name\":\"Both Classes\"}\n",
    "\n",
    "\n",
    "\n",
    "traceNew = {\n",
    "    \"x\":data1['daysOnWeb'].groupby(data1['sale_class']).get_group('New'),\n",
    "    \"type\":\"box\",\n",
    "    \"name\":\"New Class\",\n",
    "}\n",
    "traceUsed = {\n",
    "    \"x\":data1['daysOnWeb'].groupby(data1['sale_class']).get_group('Used'),\n",
    "    \"type\":\"box\",\n",
    "    \"name\":\"Used Class\",\n",
    "}\n",
    "\n",
    "fig = tools.make_subplots(rows=3, cols=1)\n",
    "fig.append_trace(traceAll,1,1)\n",
    "fig.append_trace(traceNew, 2, 1)\n",
    "fig.append_trace(traceUsed, 3, 1)\n",
    "fig['layout']['title']=\"<b>Number of Days by 'sale_class' Box Plot</b>\"\n",
    "#fig.layout = {\"title\": \"<b>Number of Days by 'sale_class' Box Plot</b>\"}\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the outcome variables is positively skewed for both classes. The median number of days until sale are 70.5 and 26 days for the new and old class respectively. Also, In the the combined classes boxplot we notice a small number of  outliers values greater than 298."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layout = go.Layout(title='Price vs. Days to sale')\n",
    "fig = go.Figure([go.Scatter(x=data1['asking_price'],y=data1['daysOnWeb'],mode = 'markers')], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scatter plot shows there are potential outliers in the data that need to be dealt with before we can proceed to modelling the data. In the following step the density based clustering algorithm DBSCAN is used to detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Standardizing the data is key for most of the clustering techniques to avoid a feature biasing the results of clustering\n",
    "\n",
    "X = StandardScaler().fit_transform(data1._get_numeric_data())\n",
    "A = []\n",
    "B = []\n",
    "C = []\n",
    "\n",
    "for i in np.linspace(0.1,5,50):\n",
    "    # we would like to eventually use this to see how the number of outliers varies as the distance variable eps increases-\n",
    "    #from 0.1 to 5\n",
    "    db = DBSCAN(eps=i, min_samples=10).fit(X)\n",
    "\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    add = 0\n",
    "    for t in labels:\n",
    "        if t == -1: \n",
    "            add = add + 1\n",
    "    C.append(add)\n",
    "            \n",
    "    \n",
    "    \n",
    "    A.append(i)\n",
    "    B.append(int(n_clusters_))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels are the label of the clusters. If the label is -1, then the observation is an outlier/noise within our dataset.\n",
    "\n",
    "db.core_sampleindices are the indices of the core points in the cluster, the indices that are excluded here are of outliers and the edges of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([A,B,C]).T\n",
    "results.columns = ['distance','Number of clusters','Number of outliers']\n",
    "results.plot(x='distance',y='Number of outliers',figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot I decided to use 1.5 as the distance variable epi since the number of outliers drop to zero after this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data first\n",
    "X = StandardScaler().fit_transform(data1._get_numeric_data().values)\n",
    "\n",
    "db = DBSCAN(eps=1.5, min_samples=10).fit(X)\n",
    "labels = db.labels_\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "for color,label in zip(colors, unique_labels):\n",
    "    sample_mask = [True if l == label else False for l in labels]\n",
    "    plt.plot(X[:,3][sample_mask], X[:, 4][sample_mask], 'o', color=color);\n",
    "plt.xlabel('Price');\n",
    "plt.ylabel('daysOnWeb');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating ouliers from the core data\n",
    "filtered_data=data1.iloc[db.core_sample_indices_, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the data with one hot encoding for our predictive algorithms\n",
    "\n",
    "filtered_data=pd.get_dummies(filtered_data)\n",
    "# Use numpy to convert to arrays\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(filtered_data['daysOnWeb'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "filtered_data= filtered_data.drop('daysOnWeb', axis = 1)\n",
    "# Saving feature names for later use\n",
    "data_list = list(filtered_data.columns)\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(filtered_data, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "transformer=StandardScaler()\n",
    "train_data=transformer.fit_transform(train_data)\n",
    "#train_labels=transformer.fit_transform(train_labels.reshape(-1, 1))\n",
    "#train_labels=maxabs_scale(train_labels, axis=0, copy=True)\n",
    "test_data=transformer.fit_transform(test_data)\n",
    "#test_labels=transformer.fit_transform(test_labels.reshape(-1, 1))\n",
    "#test_labels=maxabs_scale(test_labels, axis=0, copy=True)\n",
    "# The baseline predictions are the historical averages\n",
    "baseline_preds =  test_labels.mean()\n",
    "# Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - test_labels)\n",
    "print('Average baseline error in days: ', round(np.mean(baseline_errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compare(x_train,y_train,x_test,y_test):\n",
    "    regressors=[RandomForestRegressor(),\n",
    "                GradientBoostingRegressor(),AdaBoostRegressor(),ExtraTreesRegressor(),BaggingRegressor()]\n",
    "    \n",
    "    regressor_names=[\"RandForestRegressor\",\"GBoostRegressor\",\"AdaBoostRegressor\",\"ExTreesRegressor\",\"BaggingRegressor\"]\n",
    "    \n",
    "    metrics_arr=[]#array for storing model metrics\n",
    "    for index, regressor in enumerate(regressors):\n",
    "        regressor.fit(x_train,y_train) \n",
    "        y_pred=regressor.predict(x_test)\n",
    "        R2=round(regressor.score(x_test, y_test),3)\n",
    "        errors= y_test-y_pred\n",
    "        MAE=round(np.mean(abs(errors)), 3)\n",
    "        MSE=round(np.sum(errors**2)/len(y_test),3)\n",
    "        metrics_arr.append([regressor_names[index],R2,MAE,MSE])\n",
    "    metrics_arr=np.array(metrics_arr)    \n",
    "    return(pd.DataFrame(metrics_arr,columns=[\"Model\",\"R-squared\",\"MAE\",\"MSE\"]))\n",
    "       \n",
    "model_compare(train_data,train_labels,test_data,test_labels)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the RandomForest and the GB model are the better performing models amongst the 5 models evaluated. Lets examine the learning curves of the two models to see how they perform with various training set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tools.make_subplots(rows=1, cols=2,\n",
    "                          subplot_titles=(\"Learning Curves (GradientBoosting)\",\n",
    "                                          \"Learning Curves (RandomForest))\"))\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, colnum, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10),\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "    https://plot.ly/scikit-learn/plot-learning-curve/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if(colnum==1):\n",
    "        leg=True\n",
    "    else:\n",
    "        leg=False\n",
    "        \n",
    "    p1 = go.Scatter(x=train_sizes, y=test_scores_mean + test_scores_std,\n",
    "                    mode='lines',\n",
    "                    line=dict(color=\"green\", width=1),\n",
    "                    showlegend=False,  )\n",
    "    fig.append_trace(p1, 1, colnum)\n",
    "    \n",
    "    p2 = go.Scatter(x=train_sizes, y=test_scores_mean - test_scores_std,\n",
    "                    mode='lines',\n",
    "                    line=dict(color=\"green\", width=1),\n",
    "                    showlegend=False, fill='tonexty')\n",
    "    fig.append_trace(p2, 1, colnum)\n",
    "    \n",
    "    p3 = go.Scatter(x=train_sizes, y=train_scores_mean + train_scores_std,\n",
    "                    mode='lines',\n",
    "                    line=dict(color=\"red\", width=1),\n",
    "                    showlegend=False)\n",
    "    fig.append_trace(p3, 1, colnum)\n",
    "    \n",
    "    p4 = go.Scatter(x=train_sizes, y=train_scores_mean - train_scores_std,\n",
    "                    mode='lines',\n",
    "                    line=dict(color=\"red\", width=1),\n",
    "                    showlegend=False, fill='tonexty')\n",
    "    fig.append_trace(p4, 1, colnum)\n",
    "    \n",
    "    p5 = go.Scatter(x=train_sizes, y=train_scores_mean, \n",
    "                    marker=dict(color='red'),\n",
    "                    name=\"Training score\", showlegend=leg)\n",
    "    fig.append_trace(p5, 1, colnum)\n",
    "    \n",
    "    p6 = go.Scatter(x=train_sizes, y=test_scores_mean, \n",
    "                    marker=dict(color='green'),\n",
    "                    name=\"Cross-validation score\", showlegend=leg)\n",
    "    fig.append_trace(p6, 1, colnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "plot_learning_curve(GradientBoostingRegressor(), filtered_data,labels,1,cv=cv)\n",
    "plot_learning_curve(RandomForestRegressor(), filtered_data,labels,2,cv=cv)\n",
    "fig['layout'].update(hovermode='closest')\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=GradientBoostingRegressor().fit(train_data,train_labels) \n",
    "y_pred=regressor.predict(test_data)\n",
    "R2=regressor.score(test_data, test_labels)\n",
    "errors= test_labels-y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForest model seems to be overfitting on the trainig set and the high difference in the R-squared scores between the training and cross validation sets suggest that the model could benefit from more training data.\n",
    "\n",
    "The cross validation learning curve seems to be plateauing suggesting more features are needed to improve the model(more model complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(title='Residuas vs. Fitted')\n",
    "fig = go.Figure([go.Scatter(x=y_pred,y=errors,mode = 'markers')], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual vs.s fiited values plot shows that the magnitude of the errors increases with the inrease in predicted values (demonstrates a cone like shape). This means that the model predicts lower values of the outcome variable more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(title='Actual vs. Fitted')\n",
    "fig = go.Figure([go.Scatter(x=test_labels,y=y_pred,mode = 'markers')], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Now the importance of each feature is investigated using a feature importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_feature_importances = pd.DataFrame(regressor.feature_importances_,\n",
    "                                   index = data_list,\n",
    "                                    columns=['Importance']).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor_feature_importances[regressor_feature_importances.Importance>0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top most important features that affect the predictability of the modle are:\n",
    "\n",
    "   1. The month in which the vehicle was added to the inventory (April and May)\n",
    "   2. The number of times the vehilce was viewed on the dealership's website.\n",
    "   3. The asking price\n",
    "   4. The year in which the vehicle was manufactured \n",
    "\n",
    "It is worth mentoning that some features are implicityly included in other features for instance the sales class 'new' also corresponds to the year 2018 of the vehicles year of manufacture. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
